\chapter{Basics of Reinforcement Learning}
\label{ch:reinforcement_learning}

In the field of \ac{AI}, it has been hypothesised that intelligence can be understood as subserving the maximisation of reward~\parencite{silver_reward_2021}.
It was suggested that intelligent abilities and behaviours could be attained by agents that learn from trial-and-error by receiving feedback on their performance: the reward.
\ac{RL} is one among the possible \emph{generic} formulations aiming to train agents that solve the problem of maximising the reward.

\acl{RL} operates on a unified setting decoupled as two systems interacting sequentially over time, illustrated in Figure~\ref{fig:rl_setting}.
The \emph{environment} is the world the \emph{agent} interacts with.
Differently from other domains of \ac{ML}, the learned \ac{RL} policy generates actions that may affect not only the current instant (immediate reward), but also the new configuration in which the environment transitions and its corresponding reward.
The trial-and-error nature together with delayed rewards, which give rise to the \emph{credit assignment problem}, are two among the essential features that characterise \acs{RL}~\parencite{sutton_reinforcement_2018}.

This chapter, based on the theory and notation from~\textcite{achiam_spinning_2018} and \textcite{dong_deep_2020}, describes in greater detail the \acl{RL} setting, introduces the terminology and the mathematical framework used throughout the thesis, and presents algorithms to solve the corresponding reward maximisation problem.
Despite being a generic setting from a high-level perspective, all the element parts of Figure~\ref{fig:rl_setting} may have different properties altering the formulation of the specific problem at hand.
This thesis focuses on the application of \ac{RL} to the robotics domain, constraining the nature of environment modelling, the structure of the policies that can be learned, and the family of algorithms that can be employed.
In particular, this chapter is aimed at introducing the theory of operating on environments modelled with unknown stochastic continuous dynamics, providing continuous rewards and receiving continuous actions generated by stochastic policies implemented as neural networks.
The setting adopting this family of policies is known as \ac{DRL}.
We will use throughout the thesis the \ac{RL} and \ac{DRL} terminology interchangeably.

\begin{figure}
    \centering
    \resizebox{.65\textwidth}{!}{
    \includegraphics{images/background/rl_setting.tikz}}
    \caption{The \acl{RL} setting.}
    \label{fig:rl_setting}
\end{figure}

\section{Key concepts}
\label{section:key_concepts}

This section provides the basic terminology and definitions of \ac{RL} used throughout the following chapters, mainly borrowed from~\parencite{sutton_reinforcement_2018, achiam_spinning_2018, dong_deep_2020}.

\subsection{Environment}

The environment is the -- rather abstract -- entity of the \ac{RL} setting that determines the evolution of a system as a consequence of an action, and the generation of the reward signal.

Let's assume we can encode the overall configuration of the environment at time $t$ in a \emph{state} $s_t \in \mathcal{S}$.
If $a_t \in \mathcal{A}$ is the action generated by the agent at time $t$, we define as \emph{state transition} the map $(s_t, a_t) \mapsto s_{t+1}$.
We model the state such that the transition to a next state at time $t$ depends only on $s_t$ and the selected action.
Therefore, the state's definition is enough to encode all the information about the past evolution of the environment.

State transition maps can assume different forms depending on the nature of the environment.
Formally, if the environment is \emph{deterministic}, state transitions can be expressed with a \emph{state-transition function} $f: \mathcal{S} \times \mathcal{A} \to \mathcal{S}$; instead, if the environment is \emph{stochastic}, state transitions can be expressed with the \emph{state-transition probability density function} $\mathcal{P}: \mathcal{S} \times \mathcal{A} \to \operatorname{Pr}[\mathcal{S}]$:
%
\begin{equation}
\label{eq:state_transition_equations}
\begin{aligned}
    s_{t+1} &= f(s_t, a_t) &\text{if deterministic,}\\
    s_{t+1} &\sim \mathcal{P}(\cdot|s_t, a_t) &\text{if stochastic.}
\end{aligned}
\end{equation}

The environment is also responsible for generating the \emph{immediate reward} $r_t \in \mathbb{R}$.
In its most generic form, the \emph{reward function} can be modelled as a function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \mathbb{R}$:
%
\begin{equation*}
    r_t = \mathcal{R}(s_t, a_t, s_{t+1}) .
\end{equation*}

\acl{RL}, contrary to other approaches that find optimal policies in a comparable setting, assumes that both the state-transition function $\mathcal{P}$ and the reward function $\mathcal{R}$ are unknown.
It focuses on methods to learn how to act optimally only by sampling sequences of states-actions-rewards without assuming any knowledge of the process generating their data.

The objective(s) of the learning process are known as \emph{tasks}.
In the context of \ac{RL}, the reward function $\mathcal{R}$ is responsible to guide the agent towards the fulfilment of the considered task(s).

\begin{remark*}[Episodic and Continuing tasks]
%
The iterative nature of \ac{RL} requires the environment to be sampled continuously over different \emph{episodes}.
An \emph{episode} is defined as the environment evolution from an initial state $s_0$ to its termination.
We can identify two episode termination strategies, corresponding to the underlying \emph{task} -- the objective the reward is maximising.
\emph{Episodic tasks} are characterised by a --possibly variable-- finite length $T$, \ie they do not continue after reaching a terminal state $s_T$.
\emph{Continuing tasks}, instead, are characterised by an infinite horizon and they never terminate.
On some occasions, it is useful considering episodic tasks as continuing tasks by assuming that in time steps $t > T$, the last state $s_T$ becomes a special \emph{absorbing state}, that is a state that can only transition to itself regardless of the action generating only rewards of zero.
On other occasions, instead, it is useful to truncate the evolution of a continuing task at a certain time step~\parencite{pardo_time_2022}.
%
\end{remark*}

\begin{remark*}[States and observations]
%
It's common in the \ac{RL} literature to use the terms \emph{state} and \emph{observation} interchangeably, depending on the context.
However, practically speaking, the agent does not always receive the same state $s$ that encodes the complete environment configuration, whose space $\mathcal{S}$ could also be non-Euclidean.
When necessary, we introduce the function $O: \mathcal{S} \to \mathcal{O}$ that computes the observation $o \in \mathcal{O}$ from the state $s$.
Oftentimes, we consider the observation $o$ as the input to the agent, and it is convenient to consider $\mathcal{O} = \mathbb{R}^n$.
To keep a clean notation and reduce confusion, in the rest of this background chapter we just use the notation $s$.
We only differentiate between $s$ and $o$ when they are explicitly different quantities.
Furthermore, we always assume \emph{fully observable} problems, meaning that the full state is always available to the agent~\parencite{lovejoy_survey_1991, jaakkola_reinforcement_1994}. 
%
\end{remark*}

\subsection{Agent}

The agent is the -- rather abstract -- entity of the \ac{RL} setting that learns and interacts with the environment with the objective of maximising the received reward signal.
In our setting, depending on the \ac{RL} algorithm selected for training, agents are at least composed of a function approximator used to generate the action, \ie a \emph{policy}, and a method for optimising this function given the observed states and received rewards.

\subsection{Policy}

The policy encodes the strategy followed by the agent in order to select its control actions.
Policies can be either \emph{deterministic}, \ie given a state $s$, the agent always takes the same action:
%
\begin{equation*}
    a_t(s_t) = \mu(s_t) \quad \text{, with } \mu(s_t): \mathcal{S} \to \mathcal{A},
\end{equation*}
%
or \emph{stochastic}, \ie modelled as a probability distribution, from which the action is sampled:
%
\begin{equation*}
    a_t \sim \pi(\cdot|s_t) \quad \text{, with } \pi(\cdot|s_t): \mathcal{S} \to \operatorname{Pr}(\mathcal{A}).
\end{equation*}
%
In most \ac{DRL} applications, policies assume the form of neural networks, whose parameters $\boldsymbol{\theta}$ are updated during the training procedure by an optimisation algorithm.
We often make this parameterization explicit by using subscripts, i.e.\ $\mu_{\boldsymbol{\theta}}$ and $\pi_{\boldsymbol{\theta}}$.
%
The most common policy chosen in our target setting is described in the following example.
%
\begin{example*}[Diagonal Gaussian Policy]
%
\label{example:diagonal_gaussian_policy}
The general form for the \ac{PDF} of the univariate normal distribution is the following Gaussian function:
%
\begin{equation}
    \label{eq:gaussian_function}
    f_{\mathcal{N}}(x \given \mu, \sigma)
    = \frac{1}{\sigma \sqrt{2\pi}} \exp \left( -\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2} \right).
\end{equation}
%
We denote sampling a single real-valued random variable $x \in \mathbb{R}$ from this distribution as $x \sim \mathcal{N}(\mu, \sigma^2)$, where $\mu$ and $\sigma^2$ are the \emph{mean} and the \emph{variance} of the distribution, respectively.
We can use the same distribution also in the multivariate case, in which $x, \mu \in \realn^k$ and $\Sigma = \operatorname{diag}(\sigma_1, \sigma_2, \dots) \in \realn^{k\times k}$, where we assumed a diagonal covariance matrix parameterized with diagonal values denoted as $\sigma \in \realn^k$.
A stochastic policy could be implemented as a parameterized multivariate diagonal Gaussian distribution.
During the training phase, exploration is achieved by sampling from the distribution.
During the evaluation phase, instead, the policy's exploitation is achieved by always selecting the mean of the distribution.
We can decide to condition the action $a_t$ on the state $s_t$ by modelling the distribution with a \ac{NN} parameterized by $\boldsymbol{\theta}$.
We define the policy being the corresponding parameterized density, denoted as:
%
\begin{equation*}
    \pi_{\boldsymbol{\theta}}(a_t \given s_t) = 
    f(a_t \given \boldsymbol{\theta}(s_t)) = f_{\mathcal{N}}(a_t \given \mu_{\boldsymbol{\theta}}(s_t), \sigma_{\boldsymbol{\theta}})
    .
\end{equation*}
%
A common choice is to compute only $\mu_{\boldsymbol{\theta}} =\mu_{\boldsymbol{\theta}}(s_t) $ from the forward pass of the \ac{NN}, which takes the state $s_t$ as input.
The standard deviation is often computed by standalone parameters independent of the \ac{NN} and consequently is state-independent.
Furthermore, to prevent generating negative values of $\sigma_{\boldsymbol{\theta}}$, it's common practice to let these standalone parameters provide $\log \sigma_{\boldsymbol{\theta}}$, so that they could be optimised in $\realn$ and, upon exponentiation, return a standard deviation in $\realn^+$.

In this setting, actions could be sampled as $a_t \sim \pi_{\boldsymbol{\theta}}(\cdot \given s_t)$.
Implementations that exploit \ac{AD} frameworks modify the sampling strategy using the \emph{reparameterization trick}, allowing to apply backpropagation on stochastic problems.
In practice, actions are sampled as $a_t =
\mu_{\boldsymbol{\theta}}(s_t) + \sigma_{\boldsymbol{\theta}} \cdot \boldsymbol{z}$, with $\boldsymbol{z} \sim \mathcal{N}(0, 1)$.
%
\end{example*}

\begin{remark*}[\acl{PDF} and likelihood]
%
The Gaussian function of Equation~\eqref{eq:gaussian_function} could be used for two scopes depending on which of its parameters is considered as fixed.
When considered as $x \mapsto f(x \given \mu, \sigma)$, it defines the \ac{PDF} of the distribution.
Instead, when considered as $(\mu, \sigma) \mapsto f(x \given \mu, \sigma)$, it defines the \emph{likelihood function} of the distribution.
The former calculates the probability of $x$ to fall within a particular range of values considering constant distribution parameters.
The latter, instead, also denoted as $\mathcal{L}(\mu, \sigma \given x)$, describes the probability of the observed data $x$ as a function of varying distribution parameters.
As we will analyse later, calculations are often simpler when considering the \emph{log-likelihood} $\boldsymbol{\ell} = \log{\mathcal{L}}$ instead of the plain likelihood.
From a state $s$, assuming a diagonal multivariate Gaussian policy with mean $\mu = \mu_{\boldsymbol{\theta}}(s)$ and standard deviation $\sigma = \sigma_{\boldsymbol{\theta}}$, it can be shown that the log-likelihood of an action $a \in \mathbb{R}^k$ is the following~\parencite{bishop_pattern_2006}:
%
\begin{equation}
    \boldsymbol{\ell}(\mu, \sigma \given a) =
    \log\pi_{\boldsymbol{\theta}}(a \given s) =
    -\frac{1}{2} k \log{2\pi} - \sum_{i=1}^{k} \left( \log{\sigma_i} + \frac{(a_i - \mu_i)^2}{2 \sigma_i^2} \right).
\end{equation}
%
\end{remark*}

\subsection{Trajectory}

In the \ac{RL} setting illustrated in Figure~\ref{fig:rl_setting}, 
the sequential interaction between the agent and the environment generates a \emph{trajectory} $\tau$ of states and actions:
%
\begin{equation*}
    \tau = (s_0, a_0, s_1, a_1, \dots, s_T).
\end{equation*}
%
The first state $s_0$, \ie the starting point of the trajectory, is randomly sampled from the \emph{initial state distribution} $\rho_0(\cdot)$ as $s_0 \sim \rho_0(\cdot)$.
Afterwards, the state evolves accordingly to one of the state transition maps defined in Equation~\eqref{eq:state_transition_equations}, until a terminal state $s_T$ is reached\footnote{This is always true for episodic tasks, not always for the continuing tasks that characterise continuous control.
When a continuous control trajectory terminates on a terminal state $s_T$, there is no difference with an episodic task.
However, practically speaking, truncating a continuous control trajectory is common after a given number of steps.
It is essential to distinguish these two cases because the propagation of the reward backward in time differs significantly.}.
The process generating trajectory data is illustrated in Figure~\ref{fig:data_generating_process_mdp}.

\begin{figure}
    \centering
    \resizebox{\textwidth}{!}{
    \includegraphics{images/background/data_generating_process_mdp.tikz}
    }
    \caption{Illustration of the process generating trajectory data.}
    \label{fig:data_generating_process_mdp}
\end{figure}

\begin{remark*}[Reward trajectory]
%
The literature is not uniform on the time subscript of the immediate reward $r_{(\cdot)}$.
In this thesis, we consider the data-generating process illustrated in Figure~\ref{fig:data_generating_process_mdp}.
The transition $t \rightarrow t+1$ corresponds to the data $(s_t, a_t) \mapsto (r_t, s_{t+1})$.
Note that the subscript of the reward is associated to the time $t$ of the tuple $(s_t, a_t)$ that generated it.
While this might seem odd at a first glance, it keeps the notation consistent when building the dataset $\mathcal{D}$ of a trajectory $\tau$.
In practice, it's common to store the experience in the form $\mathcal{D} = \{(s_0, a_0, r_0), (s_1, a_1, r_1), \dots, (s_{T-1}, a_{T-1}, r_{T-1}), (s_T, \cdot, 0)\}$.
%
\end{remark*}

\subsection{Return}

The rewards $r_t = \mathcal{R}(s_t, a_t, s_{t+1})$ returned at every step by the environment have been defined as \emph{immediate}, which means generated from and related only to the transition $t \rightarrow t+1$.
From the description of the \ac{RL} setup provided in \ref{section:key_concepts}, it could be intuitive to conclude that the reward maximisation process should allow, in some way, to consider possibly delayed rewards that could occur over a trajectory.
In other words, an action taken at time $t$ in state $s_t$ may not immediately produce a high reward $r_t$, but it could lead to future states associated with high rewards.
With this intuition in mind, we can define the \emph{return} at time $t$ as the sum of all the immediate rewards received until termination:
%
\begin{equation*}
    R_t = r_t + r_{t+1} + r_{t+2} + \dots + r_T = \sum_{k=0}^{T - t} r_{t+k} .
\end{equation*}
%
This equation defines the \emph{finite-horizon undiscounted return}, which suits episodic tasks well.
However, if applied to continuing tasks, the sum could not converge to a limit value because the final time step is $T=\infty$.
In this setting, we can introduce the \emph{infinite-horizon discounted return}:
%
\begin{equation*}
    R_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k r_{t+k} .
\end{equation*}
%
The term $\gamma \in [0, 1]$ is called \emph{discount factor}, and allows to obtain a bounded return.
The discount factor is one of the most critical hyper-parameters to tune since it controls how farsighted the agent could be once trained.

Considering a trajectory $\tau = (s_0, a_0, s_1, a_1, \dots, s_{T-1}, a_{T-1}, s_T)$, we denote as $R(\tau)$ its discounted return, defined as:
%
\begin{equation*}
    R(\tau) = 
    \sum_{t=0}^{T-1} \gamma^t \mathcal{R}(s_t, a_t, s_{t+1}) = 
    \sum_{t=0}^{T-1} \gamma^t r_t .
\end{equation*}

\begin{remark*}[Variance in the bias-variance trade-off]
\label{remark:variance_in_biasvariance}
%
During the training phase, many \ac{RL} algorithms try to estimate the expected return of each state of the environment.
If we take multiple trajectories starting from the same state but evolving differently, the computed return of that state for each trajectory may differ considerably.
In fact, the return sums -- and optionally discounts -- the collected future rewards.
A possible estimation of a state's return could be obtained by averaging the computed return of all trajectories.
The variance of the estimated return constitutes one element of the
\emph{bias-variance} trade-off, that characterises many policy learning methods.
%
\end{remark*}

\subsection{The Reinforcement Learning Problem}
\label{section:reinforcement_learning_problem}

Combining all these concepts allows defining the \ac{RL} problem more pragmatically.
In our setting, it is defined as follows:
%
\begin{displayquote}
    The maximisation of the infinite-horizon discounted return $R(\tau)$ of a trajectory $\tau$ sampled following a stochastic policy $\pi(\cdot|s_t)$ from an uncertain environment modelled with an unknown state transition probability $\mathcal{P}(\cdot|s_t, a_t)$ and unknown reward function $\mathcal{R}(s_t, a_t, s_{t+1})$.
\end{displayquote}
%
In this setting, we can compute the probability of a trajectory $\tau$~\parencite{sutton_reinforcement_2018, achiam_spinning_2018} of an arbitrary length $T$ as\footnote{For continuing tasks, we can truncate the trajectory after $T$ steps and handle the return of the last state properly.}:
%
\begin{equation}
    \label{equation:probability_of_trajectory}
    P(\tau \given \pi) = \rho_0(s_0) \prod_{t=0}^{T-1} \mathcal{P}(s_{t+1} \given s_t, a_t) \pi(a_t \given s_t)
\end{equation}
%
We can now formulate the reward maximisation objective as an optimisation problem by introducing the following \emph{performance function}:
%
\begin{equation}
    \label{equation:policy_performance_function}
    J(\pi)
    = \int_\tau P(\tau \given \pi) R(\tau) \dd{\tau}
    = \E_{\tau \sim \pi} \left[ R(\tau) \right] .
\end{equation}
%
Finally, the \ac{RL} problem can be mathematically defined as follows:
%
\begin{equation*}
    \pi^* = \argmax_{\pi} J(\pi) ,
\end{equation*}
%
where $\pi^*$ is the \emph{optimal policy} yielding the maximum return from each visited state along the trajectory.

\section{Reinforcement Learning Formalism}

The previous section provided an informal introduction to the \acl{RL} setting.
In this section, we consolidate and formalise the notions into a structured framework composed of two key ingredients: \aclp{MDP} and the Bellman equation.

\subsection{Markov Decision Processes}

\aclp{MDP}~\parencite{puterman_markov_2005, sutton_reinforcement_2018} are one of the classical formulations of sequential decision-making, which introduce the mathematical framework of choice for the discrete-time stochastic setting described in Section~\ref{section:key_concepts}.

With the notation introduced in the previous section:
%
\begin{itemize}
    \item The set of all valid states $\mathcal{S}$,
    \item The set of all valid actions $\mathcal{A}$,
    \item The reward function $\mathcal{R}: \mathcal{S} \times \mathcal{A} \times \mathcal{S} \to \mathbb{R}$,
    \item The state-transition probability function $\mathcal{P}: \mathcal{S} \times \mathcal{A} \to \operatorname{Pr}[\mathcal{S}]$,
    \item The initial state distribution $\rho_0$, 
\end{itemize}
%
we define as \ac{MDP} the tuple $\langle \mathcal{S}, \mathcal{A}, \mathcal{R}, \mathcal{P}, \mathcal{\rho}_0 \rangle$.
%
The two key features of the iterative \ac{RL} settings are correctly modelled by a \ac{MDP}.
Firstly, \acp{MDP} satisfy the \emph{Markov Property}.
%
\begin{definition*}[Markov Property]
%
A stochastic process satisfies the Markov property if the conditional probability distribution of future states of the process depends only upon the present state.
Given the present state, the future does not depend on the past ones.
A process with this property is said to be Markovian or a Markov process.
%
\end{definition*}
%
It follows that the dynamics of an \ac{MDP} is uniquely modelled by $\mathcal{P}$~\parencite{sutton_reinforcement_2018}.
Given a state-action pair $(s_t, a_t) \in \mathcal{S} \times \mathcal{A}$, the probability of transitioning to the next state $s_{t+1}$ is $\mathcal{P}(s_{t+1} \given s_t, a_t)$.
Concerning the support of delayed rewards to solve the credit assignment problem, which represents the second key feature of iterative \ac{RL}, \acp{MDP} exploit \emph{value functions}.

\subsection{Value functions}
\label{section:value_functions}

Given a \acl{MDP} $\mathcal{M}$, we can associate each state $s$ -- or state-action pair $(s, a)$ -- to a scalar value representing how rewarding it is for an agent to generate a trajectory passing through it.
Considering the objective of reward maximisation, a natural choice for this score is the \emph{expected return}.
We introduce the \emph{state-value function for policy $\pi$} as the following quantity:
%
\begin{equation}
    \label{equation:state_value_function}
    V^\pi(s) 
    = \E_{\tau \sim \pi} \left[ R(\tau) \given s_0 = s \right]
    = \E_{\tau \sim \pi} \left[ \sum_{t=0}^\infty \gamma^t r_t \,\middle\vert\, s_0 = s \right] .
\end{equation}
%
It provides the expected return of a trajectory starting from $s$ and always acting following policy $\pi$.
This is also the reason why we need to specify the active policy with the superscript.

Another important value function to introduce is the \emph{action-value function for policy $\pi$}:
%
\begin{equation}
    \label{equation:action_value_function}
    Q^\pi(s, a) 
    = \E_{\tau \sim \pi} \left[ R(\tau) \given s_0 = s, a_0 = a \right]
    = \E_{\tau \sim \pi} \left[ \sum_{t=0}^\infty \gamma^t r_t \,\middle\vert\, s_0 = s, a_0 = a \right] .
\end{equation}
%
It provides the expected return of a trajectory starting from $s$, taking an action $a$ -- not necessarily generated by the same policy $\pi$ --, and then always acting following policy $\pi$.
In this case, we assign a scalar value to each state-action pair $(s, a) \in \mathcal{S} \times \mathcal{A}$.

The definitions of these value functions now give us a helpful metric to define the performance of a policy $\pi$.
In fact, we can consider policy $\pi_A$ better than policy $\pi_B$ if $V^{\pi_A}(s) > V^{\pi_B}(s)$ for all states $s \in \mathcal{S}$.
An \ac{MDP} $\mathcal{M}$ always has at least one policy that performs better than all the others, and this is the optimal policy $\pi_*$.
The optimal state-value function and the optimal action-value function of the policy $\pi_*$ (or policies, if the optimal policy is not unique) are:
%
\begin{align*}
    V^*(s) 
    &= \max_\pi V^\pi(s)
    = \max_\pi \E_{\tau \sim \pi} \left[ R(\tau) \given s_0 = s \right] \\
    Q^*(s, a)
    &= \max_\pi Q^\pi(s, a)
    = \max_\pi \E_{\tau \sim \pi} \left[ R(\tau) \given s_0 = s, a_0 = a \right] .
\end{align*}

It is often not needed to compute both value functions.
Two connections between the value functions are the following~\parencite{sutton_reinforcement_2018}:
%
\begin{align}
    V^\pi(s) &= \E_{a \sim \pi} \left[ Q^\pi(s, a) \right] \label{equation:v_as_func_of_q}\\
    Q^\pi(s, a) &= \E_{s_{t+1} \sim \mathcal{P}(\cdot \given s_t, a_t)} \left[ r_t + \gamma V^\pi(s_{t+1}) \given s_t=s, a_t=a \right] \label{equation:q_as_func_of_v}
    .
\end{align}

\begin{remark*}[Bias in the bias-variance trade-off]
\label{remark:bias_in_biasvariance}
%
Many \ac{RL} algorithms operating on continuous spaces, during the training phase, try to fit a function to estimate the optimal value function (or functions).
Before reaching an acceptable convergence, the effect of using a wrong value function estimate that could affect training performance is called \emph{bias}, and it is the other element of the previously introduced \emph{bias-variance} trade-off that characterises many policy learning methods (Remark~\ref{remark:variance_in_biasvariance}).
%
\end{remark*}

A commonly-used byproduct of the state-value and action-value functions is the \emph{advantage function}:
%
\begin{equation}
    \label{equation:advantage_function}
    A^\pi(s, a) = Q^\pi(s, a) - V^\pi(s) .
\end{equation}
%
It plays an important role when we need to describe the quality of an action in a relative sense.
In fact, we can think of $V(s_t)$ as a function providing the expected return of state $s_t$ averaged over all the possible actions $a_t$ that can be taken in this state\footnote{This can be clearly seen from Equation~\eqref{equation:v_as_func_of_q}.}, and $Q(s_t, a_t)$ as providing the expected return of state $s_t$ considering that the action taken was $a_t$.
If this action $a_t$ performs better than average, expressed mathematically as $Q(s_t, a_t) > V(s_t) \implies A(s_t, a_t) > 0$, we could use this information to reinforce the choice of $a_t$ the next time the trajectory evolves through $s_t$.

\subsection{Bellman Equation}
\label{section:bellman_equation}

An optimisation problem in discrete time like the \ac{RL} problem can be structured in a recursive form, \ie expressing the relationship between a quantity in one step and the same quantity in the next one.
If a problem can be structured in this form, the equation expressing the update rule between the two periods is known as \emph{Bellman equation}.

The value functions for policy $\pi$ introduced in Section~\ref{section:value_functions} can be transformed into a recursive form by noticing that we can express the return as follows:
%
\begin{align*}
    R_t
    &= r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots = r_t + \gamma \left( r_{t+1} + \gamma r_{t+2} + \cdots \right) \\
    &= r_t + \gamma R_{t+1} = \mathcal{R}(s_t, a_t, s_{t+1}) + \gamma R_{t+1}.
\end{align*}
%
Replacing this relation in Equation~\eqref{equation:state_value_function} and Equation~\eqref{equation:action_value_function} leads to the Bellman equations of the value functions for policy $\pi$:
%
\begin{align*}
    V^\pi(s_t) 
    &= \E_{\substack{a_t \sim \pi \\ s_{t+1} \sim \mathcal{P}}} \left[ \mathcal{R}(s_t, a_t, s_{t+1}) + \gamma V^\pi(s_{t+1}) \right] ,\\
    Q^\pi(s_t, a_t) 
    &= \E_{s_{t+1} \sim \mathcal{P}} \left[ \mathcal{R}(s_t, a_t, s_{t+1}) + \gamma \E_{a_{t+1} \sim \pi} \left[  Q^\pi(s_{t+1}, a_{t+1}) \right] \right] .
\end{align*}
%
Once the optimal policy $\pi^*$ has been found, the Bellman equations for this policy are the same except from the action selection that, instead of sampling it as $a \sim \pi(\cdot|s)$, deterministically selects the action yielding the highest value.

One of the possible solutions to problems structured in this recursive form is proceeding by \emph{backward induction}, \ie considering what is the optimal action to take in the last state of a sequence, and then propagating backward in time.
This process has a closed form under specific assumptions.
Our setting, characterised by continuous action and state spaces, unknown $\mathcal{P}$ and $\mathcal{R}$, and the usage of function approximation for the value functions and the policy, does not have any closed-form solution.
Therefore, at best, we can use iterative approaches.

\section{Algorithms}

After the emergence of \acl{DRL}, the late 2010s have seen an extensive research effort that led to a great variety of algorithms.
In this section, we provide a bird's-eye view of different families of algorithms that can iteratively solve the \ac{RL} problem.
We highlight their main properties that motivate the choice of methods used for the setting studied in this thesis.

\subsection{Model-free and Model-based}

The formulation of the \ac{RL} problem provided in Section~\ref{section:reinforcement_learning_problem} outlines that the agent has no knowledge of environment details.
The state-transition probability density function $\mathcal{P}(\cdot|s_t, a_t)$ and the reward function $\mathcal{R}(s_t, a_t, s_{t+1})$ are usually unknown.
The agent needs to explore the environment through trial-and-error while trying to learn how to maximise the collected immediate rewards.
There are, however, algorithms that assume a --possibly partial-- knowledge of environment details that can be advantageous in different circumstances.

The first major categorisation separates the algorithms in \emph{model-free} and \emph{model-based} methods, depending on whether the agent has access to a model of the environment.
Model-free methods aim to maximise the reward directly from observed data.
Instead, model-based methods exploit the knowledge of the environment model to perform planning, enabling to anticipate the direction in which the trajectory will evolve and use this information to improve the action selection.
The environment model could either be given, or learned from the observed trajectories.
In fact, the agent has access to the environment trajectories, and under the assumption of collecting enough data, it may try to learn both the environment dynamics and reward function.

Having access to the model could seem a desired improvement in most scenarios.
In settings where model learning is successful, or the model is given, model-based methods show excellent sample efficiency, one of the major downsides of model-free methods.
Still, the reality is that in the case the model is learned from data, it remains highly challenging to obtain a description of a complex environment accurate enough to be exploited by the agent.
In addition, the prediction inaccuracy introduces a strong bias in the learning process that can be exploited by the agent, resulting in sub-optimal behaviour in the actual environment.

\subsection{Value-based and Policy-based}

The second major categorisation separates the algorithms in \emph{value-based} and \emph{policy-based} methods.
Value-based methods aim to learn the value functions introduced in Section~\ref{section:value_functions}, usually $Q^\pi(s, a)$, from which a policy can be implicitly generated.
Instead, policy-based methods do not rely on any value function.
They introduce a function approximator whose parameters $\boldsymbol{\theta}$ can be iteratively optimised to maximise a performance function $J(\boldsymbol{\theta})$.

Value-based methods learn a parameterized action-value function $Q_{\boldsymbol{\theta}}(s, a)$ usually applying updates based on the Bellman equations introduced in Section~\ref{section:bellman_equation}.
From the action-value function, actions can be deterministically computed as $a^*(s) \in \argmax_a Q_{\boldsymbol{\theta}}(s, a)$.
This equation, however, shows both the limitations of this family of methods.
First, due to the action selection based on the $\argmax$, the computation could be expensive in high-dimensional discrete action spaces.
Also, they are incompatible with continuous action spaces since the maximisation expects a finite set of actions.
Second, they can only learn deterministic policies, introducing the need to be combined with proper exploration strategies that could be less effective than those implicitly implemented with stochastic policies.
On the other hand, value-based methods can reuse most of the collected data with high sampling efficiency, and the maximisation strategy allows to improve faster and with a lower variance to the optimal policy.

Policy-based methods, instead of resorting to a value function, perform an optimisation that directly targets the final aim of reward maximisation.
They represent a policy with a parameterized function, typically a state-conditioned probability distribution, that is optimised directly from collected trajectories.
Depending on the choice of the policy, these methods are suitable for continuous and high-dimensional action spaces.
In practice, they present better convergence properties by applying small incremental changes at every iteration.
Although usually slower, less efficient, and more prone to getting stuck and converging to a local optimum, learning performance could be more stable.

The separation between value-based and policy-based methods has become less and less defined in the past few years.
Many new algorithms conceptually close to the policy-based methods, do learn and take advantage of value functions.
This family of methods is known as \emph{actor-critic} methods.
They combine the broader policy support and better convergence properties of policy-based methods by using a parameterized policy, called actor, with a lower variance obtained from learning and exploiting a value function, called critic.
The learning process interleaves optimisations of the actor and the critic so that both converge towards the optimal policy and optimal value function, respectively.

\subsection{On-policy and off-policy}

The third major categorisation separates the algorithms between \emph{on-policy} and \emph{off-policy} methods.
The difference between these two methods is whether their policy is used both as \emph{behaviour policy}, used for exploring the environment, and as \emph{target policy}, used as the actual output of the optimisation problem.

Off-policy methods are capable of learning an optimal policy from experience sampled by any exploration strategy, under the assumption of visiting enough times all environment states.
This feature makes off-policy methods particularly sample efficient since the sampled experience during training always remains valid and, therefore, can be used multiple times.
Most of the algorithms belonging to this family are also value-based, thus they inherit better convergence properties albeit being more unstable.

On-policy methods usually learn a stochastic policy and use it both as behaviour and target.
These methods are mainly either policy-based or actor-critic.
Under the assumption that the policy's stochasticity is sufficient for environment exploration, on-policy methods share the properties of policy-based methods.
Contrarily to off-policy methods, during training, they expect data to be generated from the same policy, preventing the usage of old data acquired in previous optimisation epochs and therefore showing a lower sampling efficiency.
However, the most popular algorithms in this family implement techniques based on \emph{importance sampling}, enabling multiple optimisation steps per on-policy batch, mitigating the need for newly-sampled trajectories.

\section{Policy Optimization}

Most of the \acl{RL} algorithms used in robotics belong to the family of \emph{policy gradients} methods, \ie model-free policy-based on-policy methods.
In this section, we first derive how we can compute the gradient of the policy performance function $J(\pi_{\boldsymbol{\theta}})$, already introduced in Equation~\eqref{equation:policy_performance_function}, \wrt its parameterization $\boldsymbol{\theta}$ directly from the trajectories $\tau$.
Then, we introduce \ac{PPO}, a widely used algorithm that exploits a local approximation of this gradient to let $\pi_{\boldsymbol{\theta}}$ converge towards $\pi^*$.

\subsection{Policy Gradient}

Let's consider a policy $\pi_{\boldsymbol{\theta}}$ parameterized by $\boldsymbol{\theta} \in \mathbb{R}^p$.
From Equation~\eqref{equation:policy_performance_function}, its performance function can be defined as:
%
\begin{equation}
    \label{equation:policy_performance_for_policy_gradient_derivation}
    J(\pi_{\boldsymbol{\theta}}) = \E_{\tau \sim \pi_{\boldsymbol{\theta}}} [ R(\tau) ] = \int_\tau P(\tau|\boldsymbol{\theta}) R(\tau) \dd{\tau} .
\end{equation}
%
We want to derive the equation of the gradient of this performance function \wrt $\boldsymbol{\theta}$ (or, at least, its stochastic estimate) so that we can maximise the return by optimising $\boldsymbol{\theta}$ through gradient ascent with the following update rule:
%
\begin{equation}
    \boldsymbol{\theta}_{k+1} =
    \boldsymbol{\theta}_k + \alpha \nabla_{\boldsymbol{\theta}} J(\pi_{\boldsymbol{\theta}})\Bigr|_{\substack{\boldsymbol{\theta}_k}} ,
\end{equation}
%
where $\nabla_{\boldsymbol{\theta}} J(\pi_{\boldsymbol{\theta}}) = \left[ \pdv{J}{\theta_1}, \pdv{J}{\theta_2}, \pdv{J}{\theta_3}, \cdots \right]^\top \in \realn^p$ is the \emph{policy gradient} term.
It can be expanded as:
%
\begin{align*}
    \nabla_{\boldsymbol{\theta}} J(\pi_{\boldsymbol{\theta}})
    &= \nabla_{\boldsymbol{\theta}} \int_\tau R(\tau) P(\tau \given \boldsymbol{\theta}) \dd{\tau}
    = \int_\tau R(\tau) \nabla_{\boldsymbol{\theta}} P(\tau \given \boldsymbol{\theta}) \dd{\tau} \\
    &= \int_\tau R(\tau) P(\tau \given \boldsymbol{\theta}) \nabla_{\boldsymbol{\theta}} \log P(\tau \given \boldsymbol{\theta}) \dd{\tau} \\
    &= \E_{\tau \sim \pi_{\boldsymbol{\theta}}} \left[ R(\tau) \nabla_{\boldsymbol{\theta}} \log P(\tau \given \boldsymbol{\theta}) \right]
    .
\end{align*}
%
Note that we used the Leibniz integral rule for differentiation under the integral sign to move the gradient, and the \emph{log-derivative trick} on $\nabla_{\boldsymbol{\theta}} P(\tau|\boldsymbol{\theta})$ to express it as an expectation.
%
\begin{theorem}[Log-derivative trick]
    Given a function $f(\boldsymbol{x})$, we can express its gradient in the following form:
    %
    \begin{equation}
        \label{equation:log_derivative_trick}
        \nabla_{\boldsymbol{x}} f(\boldsymbol{x}) = f(\boldsymbol{x}) \nabla_{\boldsymbol{x}} \log f(\boldsymbol{x}) .
    \end{equation}
    %
    Although it seems to involve more terms, this form can be a convenient expression since it can be easier to differentiate the logarithm of a function rather than the function itself.
    This could occur when $f$ is a product of many terms, which is transformed into a sum by taking the logarithm.
\end{theorem}
\begin{proof}
    Given a function $f(\boldsymbol{x})$, from the chain rule it follows:
    %
    \begin{equation*}
        \nabla_{\boldsymbol{x}} \log f(\boldsymbol{x}) = \frac{1}{f(\boldsymbol{x})} \nabla_{\boldsymbol{x}} f(\boldsymbol{x}) .
    \end{equation*}
    %
    Rearranging the terms of this expression leads to Equation~\eqref{equation:log_derivative_trick}.
\end{proof}
%
The application of the log-derivative trick has another important consequence.
The probability of a trajectory $\tau$ was already defined in Equation~\eqref{equation:probability_of_trajectory} as:
%
\begin{equation*}
    P({\tau \given \boldsymbol{\theta}}) =
    \rho_0(s_0) \prod_{t=0}^{T-1} \mathcal{P}(s_{t+1} \given s_t, a_t) \pi_{\boldsymbol{\theta}}(a_t \given s_t) .
\end{equation*}
%
If we take its logarithm, we obtain:
%
\begin{equation*}
    \log P({\tau \given \boldsymbol{\theta}}) =
    \log \rho_0(s_0) + \sum_{t=0}^{T-1} \left[ \log \mathcal{P}(s_{t+1} \given s_t, a_t) + \log \pi_{\boldsymbol{\theta}}(a_t \given s_t) \right] .
\end{equation*}
%
We note that it depends on $\boldsymbol{\theta}$ only through $\pi_{\boldsymbol{\theta}}(a_t \given s_t)$, therefore we can ignore the other terms and simplify the policy gradient to its final form:
%
\begin{align}
    \label{equation:policy_gradient_final}
    \nabla_{\boldsymbol{\theta}} J(\pi_{\boldsymbol{\theta}})
    &= \E_{\tau \sim \pi_{\boldsymbol{\theta}}} \left[ R(\tau) \nabla_{\boldsymbol{\theta}} \log P(\tau \given \boldsymbol{\theta}) \right] \nonumber \\
    &= \E_{\tau \sim \pi_{\boldsymbol{\theta}}} \left[ R(\tau) \sum_{t=0}^{T-1} \nabla_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}}(a_t \given s_t) \right] .
\end{align}
%
Being an expectation, we can obtain a gradient estimate by collecting at time $k$ a dataset of on-policy trajectories $\mathcal{D}_k$ and computing the \emph{empirical average}, denoted by $\hat{\E}$, over the finite batch of samples:
\begin{equation}
    \label{equation:policy_gradient_estimate_}
    \hat{g}_k
    = \hat{\E}_t \left[ R(\tau) \nabla_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}}(a_t \given s_t) \right]
    = \frac{1}{\abs{\mathcal{D}_k}} \sum_{\tau \in \mathcal{D}_k} R(\tau) \sum_{t=0}^{T-1} \nabla_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}}(a_t \given s_t)\Bigr|_{\substack{\boldsymbol{\theta}_k}} .
\end{equation}
%
\begin{remark*}[Gradient variance reduction exploiting causality]
%
The return $R(\tau)$ of Equation~\eqref{equation:policy_gradient_final} can be thought of as the weight of the log-likelihoods computed along the trajectory.
It can be noted that log-likelihoods at $t > 0$ are weighted by the return computed from $t=0$, which can be seen as reinforcing an action using a quantity that includes information from the past.
Instead, we should consider only the consequences of an action.
We can update the policy gradient with this intuition obtaining the \emph{reward-to-go policy gradient}:
%
\begin{equation}
    \label{equation:policy_gradient_reward_to_go}
    \nabla_{\boldsymbol{\theta}} J(\pi_{\boldsymbol{\theta}})
    = \E_{\tau \sim \pi_{\boldsymbol{\theta}}} \left[ \sum_{t=0}^{T-1} \nabla_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}}(a_t \given s_t) \sum_{t'=t}^{T-1} \mathcal{R}(s_{t'}, a_{t'}, s_{t'+1}) \right] .
\end{equation}
%
Now, log-likelihoods are weighted with the causal return, often called \emph{reward-to-go}:
%
\begin{equation*}
    \hat{R}_t = \sum_{t'=t}^{T-1} \mathcal{R}(s_{t'}, a_{t'}, s_{t'+1}) .
\end{equation*}
%
\end{remark*}

\subsection{Generalized Advantage Estimation}
\label{sec:gae}

Policy gradient methods are not uniquely defined by the final forms of Equation~\eqref{equation:policy_gradient_final} and Equation~\eqref{equation:policy_gradient_reward_to_go}.
They are just two specific cases of a more general formulation expressed in the following form:
%
\begin{equation}
    \label{equation:policy_gradient_generic}
    \nabla_{\boldsymbol{\theta}} J(\pi_{\boldsymbol{\theta}})
    = \E_{\tau \sim \pi_{\boldsymbol{\theta}}} \left[ \sum_{t=0}^{\infty} \nabla_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}}(a_t \given s_t) \Psi_t \right] .
\end{equation}
%
Among all possible choices of $\Psi_t$, the one yielding the lowest variance~\parencite{schulman_high-dimensional_2018} is the \emph{advantage function}, already introduced in Equation~\eqref{equation:advantage_function}:
%
\begin{equation*}
    \Psi_t = A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t) .
\end{equation*}
%
Using the advantage as log-likelihood's weight can be intuitively understood from its properties to describe the relative quality of actions.
In fact, if $A^\pi(s, a) > 0$, the policy gradient $\hat{g}$ pushes the parameters $\boldsymbol{\theta}$ towards an increased action likelihood $\pi_{\boldsymbol{\theta}}$, with the consequence of reinforcing the probability to choose again $a_t$ over other actions.
%
The main reason for the favourable properties regarding the variance reduction introduced by using the advantage function originates from integrating a reinforcement effect based on a quantity that expresses relative feedback instead of an absolute value that could be noisy.

In practice, the advantage function is unknown, and we have to find a way to obtain a valid \emph{advantage estimator} $\hat{A}(s, a)$.
There are many methods to obtain $\hat{A}_t$, that can be determined by how the return is estimated: high-variance Monte-Carlo on one end, high-bias one-step TD on the other:
%
\begin{align*}
    \hat{A}_t^{(1)} &= -V(s_t) + r_t + \gamma V(s_{t+1}) \quad& \text{one-step TD}, \\
    \hat{A}_t^{(\infty)} &= -V(s_t) + \sum_{t=0}^{\infty} \gamma^t r_t \quad& \text{Monte-Carlo}.
\end{align*}
%
Note that $\hat{A}_t^{(1)}$ is also called \emph{TD error}, denoted by $\delta_t^V$.
As done by TD methods, the process of approximating future rewards with an estimated return is also known as \emph{boostrapping} and, similarly as we discussed in Remark~\ref{remark:bias_in_biasvariance}, it introduces bias.

One may notice that we can blend the two methods by truncating the Monte-Carlo estimate after $k$ steps, and approximate the future rewards from the return corresponding to the $k$-th state.
This approach, known as $k$-step TD, allows balancing the bias-variance trade-off by interpolating between the two extremes:
%
\begin{equation*}
    \hat{A}_t^{(k)} = -V(s_t) + r_t + \gamma r_{t+1} + \cdots + \gamma^{k-1} r_{t+k-1} + \gamma^k V(s_{t+k}) .
\end{equation*}
%
We could think of $k$ as an additional hyperparameter to tune.
Intuitively, the variance is reduced by considering a smaller number of noisy sampled rewards, and the bias is mitigated by the high discount applied to the bootstrapped value.

While $k$-step TD already helps trade off bias and variance, it selects just one horizon.
More sophisticated methods compute $\hat{A}_t^{(k)}$ on multiple horizons and then combine the estimates, for example, by averaging them.
The \ac{GAE}~\parencite{schulman_high-dimensional_2018} is defined as an exponentially-weighted average of $k$-step TD estimates:
%
\begin{align*}
    \hat{A}_t^{\text{GAE}(\gamma, \lambda)} 
    &= (1 - \lambda) \left( \hat{A}_t^{(1)} + \lambda \hat{A}_t^{(2)} + \lambda^2 \hat{A}_t^{(3)} + \cdots \right) \\
    &= \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta^V_{t+l}
    .
\end{align*}
%
We can recognise $\text{GAE}(\gamma, 0) = \hat{A}_t^{(1)}$ and $\text{GAE}(\gamma, 1) = \hat{A}_t^{(\infty)}$.
The hyperparameter $0 < \lambda < 1$ balances between these two extremes.

\subsection{Proximal Policy Optimization}
\label{sec:ppo}

In the previous sections, we obtained the generic Equation~\eqref{equation:policy_gradient_generic} of the policy gradient, whose advantage-based form can be empirically estimated over a finite batch of samples as follows:
%
\begin{equation*}
    \hat{g} = \hat{\E}_t \left[ \nabla_{\boldsymbol{\theta}} \log \pi_{\boldsymbol{\theta}}(a_t \given s_t) \hat{A}_t \right] .
\end{equation*}
%
In practice, implementations of \ac{PG} use \acl{AD} frameworks to calculate a loss function and, from it, extract the gradient.
The loss function typically used is the following:
%
\begin{equation}
    \label{equation:policy_gradient_loss_advantage}
    L^{PG}(\boldsymbol{\theta}) = \hat{\E}_t \left[ \log \pi_{\boldsymbol{\theta}}(a_t \given s_t) \hat{A}_t \right] .
\end{equation}
%
Given a dataset of trajectories $\mathcal{D}$, one might think to perform multiple steps of gradient ascent.
Often, this process generates steps too large in the policy's parameters space, leading to updates that destroy the previously learned behaviour.

Authors of~\parencite{schulman_trust_2017} have proposed 
a modification of Equation~\eqref{equation:policy_gradient_loss_advantage} that guarantees the monotonic improvement of a stochastic policy.
The \ac{TRPO} algorithm exploits importance sampling for correcting the probabilities of trajectories produced by different policies, and limits how much these policies can change between two iterations using a hard constraint on their \ac{KL} divergence:
%
\begin{align*}
    \mathop{\text{maximize}}_{\boldsymbol{\theta}} \;\; & \hat{\E}_t \left[ \frac{\pi_{\boldsymbol{\theta}}(a_t \given s_t)}{\pi_{\boldsymbol{\theta}_{old}}(a_t \given s_t)} \hat{A}_t \right] \\
    \text{subject to} \;\; & \hat{\E}_t \left[ D_{KL}\left[\pi_{\boldsymbol{\theta}}(\cdot|s_t) || \pi_{\boldsymbol{\theta}_{old}}(\cdot|s_t)\right] \right] \leq \delta.
\end{align*}
%
This algorithm, in practice, relies on the computation of a linear approximation of the objective, and a quadratic approximation of the constraint.

\ac{PPO}~\parencite{schulman_proximal_2017} builds upon similar intuitions,
replacing the complicated computation of the approximated constrained problem with techniques that, although less rigorous, are much simpler and effective in practice.
Authors provide two different variants of the algorithm: \emph{clipped surrogate objective} and \emph{adaptive KL penalty coefficient}.
The two approaches can be either considered as standalone methods or combined.

The clipped variant of the algorithm removes the hard constraint of \acs{TRPO}.
Denoting the likelihood ratio as $r_t(\boldsymbol{\theta}) = \frac{\pi_{\boldsymbol{\theta}}(a_t \given s_t)}{\pi_{\boldsymbol{\theta}_{old}}(a_t \given s_t)}$, this variant modifies the loss function as follows:
%
\begin{equation*}
    L^{CLIP}(\boldsymbol{\theta}) = \hat{\E}_t \left[ \min \left\{ r_t(\boldsymbol{\theta}) \hat{A}_t, \; \operatorname{clip} (r_t(\boldsymbol{\theta}), 1 - \epsilon, 1+\epsilon) \hat{A}_t \right\} \right] .
\end{equation*}
%
While optimising over a dataset $\mathcal{D}$, after the optimisation of the first batch, $\boldsymbol{\theta} \neq \boldsymbol{\theta}_{old}$.
Clipping the likelihood ratio in the interval $1 \pm \epsilon$ has the effect of dampening potentially large steps towards the reinforcement direction ($r_t(\boldsymbol{\theta}) \gg 1$ if $\hat{A}_t>0$, and $r_t(\boldsymbol{\theta}) \ll 1$ if $\hat{A}_t < 0$).

The penalty variant of the algorithm, instead, transforms the hard constraint of \ac{TRPO} into a soft constraint:
%
\begin{equation*}
    L^{KLPEN}(\boldsymbol{\theta}) = \hat{\E}_t \left[ r_t(\boldsymbol{\theta}) \hat{A}_t - \beta D_{KL}\left[\pi_{\boldsymbol{\theta}_{old}}(\cdot|s_t) \,||\, \pi_{\boldsymbol{\theta}}(\cdot|s_t)\right] \right] .
\end{equation*}
%
While selecting a constant $\beta$ is a possibility, the authors of \ac{PPO} proposed an adaptive parameter update.
After each policy update, the \ac{KL} divergence of the target policy $\pi_{\boldsymbol{\theta}}$ from the behaviour policy $\pi_{\boldsymbol{\theta}_{old}}$ can be computed (or, if the parameterized policy has no closed-form expression, estimated) as:
%
\begin{equation*}
    d = D_{KL}\left[\pi_{\boldsymbol{\theta}_{old}}(\cdot|s_t) \, || \, \pi_{\boldsymbol{\theta}}(\cdot|s_t)\right]
    .
\end{equation*}
%
Then, the new $\beta$ parameter used in the following policy is adjusted as follows:
%
\begin{align*}
    &\beta \leftarrow \beta / 2 \text{,\quad if\;} d < d_{targ} / 1.5 \\
    &\beta \leftarrow \beta \times 2 \text{,\quad if\;} d > d_{targ} \times 1.5 .
\end{align*}
%
The constant $d_{targ}$ is the desired value of the \ac{KL} divergence.
